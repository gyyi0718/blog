import os
import time
import shutil
import zipfile
import requests
import pandas as pd
import pdfplumber
import xml.etree.ElementTree as ET
import re
from typing import List, Tuple
from pathlib import Path
from collections import Counter
from urllib.parse import urljoin, unquote
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from openpyxl import load_workbook
import openpyxl
from selenium.webdriver.common.action_chains import ActionChains
import csv
import sys
import zipfile
import shutil
import xml.etree.ElementTree as ET
import pdfplumber
from urllib.parse import unquote

os.system('')                # Virtual Terminal í™œì„±í™” (Windows 10 ì´ìƒ)
sys.stdout.reconfigure(encoding='utf-8')



class SiteConfig:
    """
    Configuration for a single site to crawl.
    - list_url_template: URL with a {page} placeholder for pagination.
    - detail_base: Base URL to resolve relative detail links.
    - row_selector: CSS selector for rows on the list page.
    - title_selector: CSS selector within a row to get the title/link.
    - attach_selector: CSS selector on detail page for attachment links.
    - pages: iterable of page numbers to crawl.
    - output_csv: filename for final report.
    """
    def __init__(
        self,
        name: str,
        list_url_template: str,
        detail_base: str,
        row_selector: str,
        title_selector: str,
        attach_selector: str,
        pages=range(1, 6),
        output_csv="usage_report.csv"
    ):
        self.name = name
        self.list_url_template = list_url_template
        self.detail_base = detail_base
        self.row_selector = row_selector
        self.title_selector = title_selector
        self.attach_selector = attach_selector
        self.pages = pages
        self.output_csv = output_csv


class SiteParser:
    """
    Base parser: override methods for site-specific extraction rules.
    """
    def __init__(self, config: SiteConfig):
        self.config = config

    def extract_post_links(self, html: str):
        raise NotImplementedError

    def extract_attachment_links(self, html: str):
        raise NotImplementedError


class DefaultParser(SiteParser):
    """
    Default parser assumes similar structure across sites.
    """
    def extract_post_links(self, html: str):
        soup = BeautifulSoup(html, 'html.parser')
        rows = soup.select(self.config.row_selector)
        links = []
        for row in rows:
            a = row.select_one(self.config.title_selector)
            if a and a.get('href'):
                links.append(urljoin(self.config.detail_base, a['href']))
        return links

    def extract_attachment_links(self, html: str):
        soup = BeautifulSoup(html, 'html.parser')
        attachments = []
        for a in soup.select(self.config.attach_selector):
            href = a.get('href')
            name = a.get_text(strip=True) or Path(href).name
            if href:
                attachments.append((urljoin(self.config.detail_base, href), name))
        return attachments

class GeumcheonParser(SiteParser):
    row_selector = 'table.p-table.simple tbody tr'
    title_selector = 'td.p-subject a'
    attach_selector = 'a.p-attach__link'

    def extract_post_links(self, html: str) -> List[str]:
        soup = BeautifulSoup(html, "html.parser")
        rows = soup.select(self.row_selector)
        links = []
        for row in rows:
            a_tag = row.select_one(self.title_selector)
            if not (a_tag and a_tag.get('href')):
                continue
            # ì„ í–‰ ìŠ¬ë˜ì‹œ ì œê±°
            href = a_tag['href']
            # ê¸°ë³¸ detail_base: "https://www.geumcheon.go.kr"
            # href ê°€ "selectBbsNttView.do?..." í˜•íƒœë¼ë©´ portal/ ì„ ë¶™ì—¬ì„œ ì ˆëŒ€ê²½ë¡œ ìƒì„±

            if href.startswith('/'):
                full_url = urljoin(self.config.detail_base, href)
            else:
                full_url = urljoin(self.config.detail_base + '/portal/', href.lstrip('/'))
            links.append(full_url)
        return links

    def extract_attachment_links(self, html: str) -> List[Tuple[str, str]]:
        soup = BeautifulSoup(html, "html.parser")
        files = []
        for a in soup.select(self.attach_selector):
            href = a.get('href')
            name = a.get_text(strip=True)
            if not href:
                continue
            href = href.lstrip('/')
            full_url = f"{self.config.detail_base}/{href}"
            files.append((full_url, name))
        return files
class MapoParser(SiteParser):
    def __init__(self, config: SiteConfig):
        super().__init__(config)
        self.detail_base = "https://www.mapo.go.kr"
        # ê²Œì‹œíŒ ë¦¬ìŠ¤íŠ¸ì—ì„œ <tr> ì„ ê³¨ë¼ë‚´ëŠ” ì…€ë ‰í„°
        self.row_selector   = 'ul.bbs_list li'       # ì‹¤ì œ list í˜ì´ì§€ êµ¬ì¡°ì— ë§ì¶° ì£¼ì„¸ìš”
        self.title_selector = 'a.bbs_tit'                  # ë¦¬ìŠ¤íŠ¸ì—ì„œ ê²Œì‹œê¸€ ë§í¬ ê³¨ë¼ë‚´ê¸°
        self.attach_selector = 'div.bbs_view_file a.file_name'

    def extract_post_links(self, html: str) -> List[str]:
        """ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ /site/main/board/expense/ìˆ«ìâ€¦ ë§í¬ë§Œ ê³¨ë¼ì„œ ë°˜í™˜"""
        soup = BeautifulSoup(html, "html.parser")
        links: List[str] = []
        # hrefê°€ '/site/main/board/expense/ìˆ«ì' í˜•íƒœì¸ <a> íƒœê·¸ë§Œ ì„ íƒ
        for a in soup.find_all("a", href=re.compile(r"^/site/main/board/expense/\d+")):
            href = a["href"]
            full = urljoin(self.config.detail_base, href)
            links.append(full)
        return links

    def extract_attachment_links(self, html: str) -> List[Tuple[str, str]]:
        """ë””í…Œì¼ í˜ì´ì§€ì—ì„œ class="file_name"ì¸ ì²¨ë¶€íŒŒì¼ë§Œ ì¶”ì¶œ"""
        soup = BeautifulSoup(html, "html.parser")
        files: List[Tuple[str, str]] = []
        for a in soup.select("a.file_name"):
            href = a.get("href")
            name = a.get_text(strip=True)
            if not href:
                continue
            full = urljoin(self.config.detail_base, href)
            files.append((full, name))
        return files

class GwangjuParser(SiteParser):
    def extract_post_links(self, html: str) -> List[str]:
        soup = BeautifulSoup(html, "html.parser")
        rows = soup.select("table.bod_list tbody tr")
        links = []
        for row in rows:
            a = row.select_one("td.list_tit a")
            if a:
                onclick = a.get("onclick", "")
                bidx = self.extract_bidx_from_onclick(onclick)
                if bidx:
                    detail_url = f"https://www.gjcity.go.kr/portal/bbs/view.do?bIdx={bidx}&ptIdx=53&mId=0311000000"
                    links.append(detail_url)
        return links

    def extract_attachment_links(self, html: str) -> List[Tuple[str, str]]:
        # ì´ê±´ seleniumìœ¼ë¡œ ì²˜ë¦¬í•  ê±°ë¼ ì—¬ê¸°ì„  ì‚¬ìš© X
        return []

    def extract_bidx_from_onclick(self, onclick_str: str) -> str:
        try:
            return onclick_str.split("'")[3]
        except IndexError:
            return ""

    def selenium_download_attachments(self, driver, download_dir: Path) -> List[Path]:
        time.sleep(1)
        files = []
        soup = BeautifulSoup(driver.page_source, "html.parser")
        file_links = soup.select("ul.file_list li a.download")
        for a in file_links:
            onclick = a.get("onclick", "")
            if "download" in onclick:
                try:
                    parts = onclick.split("'")
                    atchFileId = parts[1]
                    fileSn = parts[3]
                    download_url = f"https://www.gjcity.go.kr/common/file/download.do?atchFileId={atchFileId}&fileSn={fileSn}"
                    filename = a.get_text(strip=True)
                    filename = re.sub(r'\[\d+(\.\d+)?MB\]', '', filename)  # ì¤‘ê°„ì— ë¶™ì€ ê²½ìš°
                    filename = re.sub(r'(\.\w+)\[\d+(\.\d+)?MB\]$', r'\1', filename)  # í™•ì¥ì ë’¤ì— ë¶™ì€ ê²½ìš°

                    out_path = download_dir / filename

                    resp = requests.get(download_url)
                    with open(out_path, "wb") as f:
                        f.write(resp.content)
                    print(f"ğŸ“¥ Downloaded: {out_path}")
                    files.append(out_path)
                except Exception as e:
                    print(f"âŒ Error downloading: {e}")
        return files


class Crawler:
    """
    Core crawler: fetch list pages, detail pages, download attachments, extract data.
    """
    def __init__(self, config: SiteConfig, parser: SiteParser, download_dir: str = 'temp'):
        self.config = config
        self.parser = parser
        self.download_dir = Path(download_dir) / config.name
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.counter = Counter()

        chrome_opts = Options()
        chrome_opts.add_argument('--headless')
        self.driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=chrome_opts
        )

    def fetch_html(self, url: str) -> str:
        try:
            response = requests.get(url)
            response.encoding = 'utf-8'
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {url} - {e}")
            return ""

    def download_file(self, url: str, name_hint: str = "") -> Path:
        resp = requests.get(url, stream=True, timeout=10)
        resp.raise_for_status()

        # 1) Content-Dispositionì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
        cd = resp.headers.get("Content-Disposition", "")
        fname = None
        if "filename=" in cd:
            raw = cd.split("filename=")[-1].strip().strip('"')
            raw_bytes = raw.encode("latin1", errors="ignore")
            for enc in ("utf-8", "cp949"):
                try:
                    decoded = raw_bytes.decode(enc)
                    fname = decoded
                    break
                except Exception:
                    continue

        # 2) íŒíŠ¸ë‚˜ URLì—ì„œ ì¶”ì¶œ
        if not fname and name_hint:
            fname = name_hint
        if not fname:
            fname = Path(url).name


        # 3) ìš©ëŸ‰ ì •ë³´ ì œê±° (ì˜ˆ: "íŒŒì¼ëª….pdf[0.16MB]")
        fname = re.sub(r'\[\d+(\.\d+)?MB\]$', '', fname)

        # 4) íŠ¹ìˆ˜ë¬¸ì, ì—°ì† ê³µë°± ë“± ì œê±°
        fname = re.sub(r'[\x00-\x1f\x7f]', "", fname)
        fname = re.sub(r"\s+", " ", fname).strip()
        fname = re.sub(r'[\\/:*?"<>|]+', "_", fname)
        fname = re.sub(r'[;_]+$', "", fname)
        # 3) ìš©ëŸ‰ ì •ë³´ ì œê±°: [...MB] í˜•ì‹ ì œê±° (.pdf[0.16MB], [0.16MB].pdf ëª¨ë‘ í¬í•¨)
        fname = re.sub(r'\[\d+(\.\d+)?MB\]', '', fname)  # ì¤‘ê°„ì— ë¶™ì€ ê²½ìš°
        fname = re.sub(r'(\.\w+)\[\d+(\.\d+)?MB\]$', r'\1', fname)  # í™•ì¥ì ë’¤ì— ë¶™ì€ ê²½ìš°

        out_path = self.download_dir / fname
        with open(out_path, "wb") as f:
            for chunk in resp.iter_content(8192):
                f.write(chunk)

        size_mb = out_path.stat().st_size / 1024 / 1024
        print(f"ğŸ“¥ Downloaded: {out_path}[{size_mb:.2f}MB]")
        return out_path
    def clean_name(self, raw: str) -> str:
        """
        1) í°ë”°ì˜´í‘œ ì œê±°
        2) ì§€ì—­ëª…(ì„œìš¸ì‹œ, ì„œìš¸, ê¸ˆì²œêµ¬, ê¸ˆì²œ, ë¶€ì‚°ì‹œ, ë¶€ì‚°, í•´ìš´ëŒ€êµ¬, í•´ìš´ëŒ€, ìˆ˜ì˜êµ¬, ìˆ˜ì˜) ì „ë¶€ ì œê±°
        3) ì‰¼í‘œë‚˜ ë°±ìŠ¬ë˜ì‹œ ë’¤ ì •ë³´ ì‚­ì œ
        4) ì†Œê´„í˜¸ ë° ë‚´ë¶€ í…ìŠ¤íŠ¸ ì‚­ì œ
        5) ì˜¨ì (.) ì œê±°
        6) ëª¨ë“  ê³µë°±(ìŠ¤í˜ì´ìŠ¤, íƒ­, ì¤„ë°”ê¿ˆ) ì œê±°
        """
        s = raw.replace('"', '').strip()

        # (2) ì§€ì—­ëª… ì œê±° â€” \b ëŒ€ì‹  ê·¸ëƒ¥ ì—°ì† íŒ¨í„´ìœ¼ë¡œ
        s = re.sub(r'(ì„œìš¸ì‹œ|ì„œìš¸|ê¸ˆì²œêµ¬|ê¸ˆì²œ|ë¶€ì‚°ì‹œ|ë¶€ì‚°|í•´ìš´ëŒ€êµ¬|í•´ìš´ëŒ€|ìˆ˜ì˜êµ¬|ìˆ˜ì˜)', '', s)

        # (3) ì‰¼í‘œë‚˜ ë°±ìŠ¬ë˜ì‹œ ë’¤ ì •ë³´ ì‚­ì œ
        s = re.sub(r'[,\\].*$', '', s)

        # (4) ì†Œê´„í˜¸ ì•ˆ í…ìŠ¤íŠ¸ ì‚­ì œ
        s = re.sub(r'\([^)]*\)', '', s)

        # (5) ì˜¨ì  ì œê±°
        s = s.replace('.', '')

        # (6) ëª¨ë“  ê³µë°± ì œê±°
        s = re.sub(r'\s+', '', s)

        return s
    def normalize_restaurant_name(name: str, address: str, region: str) -> str:
        """ì‹ë‹¹ëª… ë¬¸ìì—´ì„ ì •ê·œí™”í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì£¼ì†Œ/ë¶„ì /ë¶€ê°€ ì •ë³´ë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ ì‹ë‹¹ëª…ìœ¼ë¡œ ë°˜í™˜"""
        # 1. ì§€ì—­ëª… í¬í•¨ ì£¼ì†Œ ë¶€ë¶„ ì œê±° (ì˜ˆ: "ì„œìš¸ì‹œ ê¸ˆì²œêµ¬ ..." ì œê±°)
        idx = -1
        # ì§€ì—­ë³„ ë„ì‹œ ì´ë¦„ íŒ¨í„´ ì„¤ì •
        if region.endswith('êµ¬'):
            if region in ['í•´ìš´ëŒ€êµ¬', 'ìˆ˜ì˜êµ¬']:
                city_keywords = ['ë¶€ì‚°ê´‘ì—­ì‹œ', 'ë¶€ì‚°ì‹œ', 'ë¶€ì‚°']
            elif region == 'ê¸ˆì²œêµ¬':
                city_keywords = ['ì„œìš¸íŠ¹ë³„ì‹œ', 'ì„œìš¸ì‹œ', 'ì„œìš¸']
            else:
                city_keywords = ['ì„œìš¸íŠ¹ë³„ì‹œ', 'ì„œìš¸ì‹œ', 'ì„œìš¸', 'ë¶€ì‚°ê´‘ì—­ì‹œ', 'ë¶€ì‚°ì‹œ', 'ë¶€ì‚°']
            # "ë„ì‹œ ì§€ì—­" íŒ¨í„´ íƒìƒ‰
            for city in city_keywords:
                phrase = f"{city} {region}"
                if phrase in name:
                    idx = name.index(phrase)
                    break
        # ë„ì‹œëª…ì„ í¬í•¨í•˜ì§€ ì•Šê³  ì§€ì—­ëª…ë§Œ ìˆëŠ” ê²½ìš°ë„ ì²˜ë¦¬
        if idx == -1:
            if region + ' ' in name:  # ì§€ì—­ëª… ë’¤ì— ê³µë°±ì´ ìˆëŠ” ê²½ìš°
                idx = name.index(region + ' ')
            elif name.endswith(region):  # ì§€ì—­ëª…ìœ¼ë¡œ ì´ë¦„ì´ ëë‚˜ëŠ” ê²½ìš°
                idx = name.index(region)
        # ì‹ë‹¹ëª…ê³¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ì§€ì—­ëª…ì¼ ë•Œë§Œ ì˜ë¼ëƒ„
        if idx != -1:
            if idx == 0 or name[idx - 1].isspace():
                name = name[:idx]
        # 2. ê´„í˜¸ë¡œ ëœ ë¶€ê°€ì •ë³´ ì œê±° (ëŒ€í‘œìëª… ë“±)
        name = re.sub(r'\([^)]*\)', '', name)
        # 3. ì¸µìˆ˜ ì •ë³´ ì œê±° (ì˜ˆ: "1ì¸µ" ë“±ì˜ íŒ¨í„´)
        name = re.sub(r'\s*\d+ì¸µ$', '', name)
        # 4. ë¶„ì  í‘œì‹œ ì œê±° (ìˆ«ì ë° "ê´€"/"í˜¸ì " ë˜ëŠ” "ë³¸ì "ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°)
        name = re.sub(r'\s*(?:\d+\s*(?:ê´€|í˜¸ì )?|ë³¸ì )$', '', name)
        # 5. ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬ (ì–‘ìª½ ê³µë°± ì œê±° ë° ì¤‘ë³µ ê³µë°± ì¶•ì•½)


        name = ' '.join(name.split())
        return name

    def reaggregate_csv(self, input_csv: str, output_csv: str):
        """
        input_csv ì— ìˆëŠ” 'place, count' ë°ì´í„°ë¥¼
        ì£¼ì†Œì˜ ì‰¼í‘œëŠ” ë¬´ì‹œí•˜ê³  ë§¨ ë§ˆì§€ë§‰ ì‰¼í‘œë§Œ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•´
        place ë¥¼ ì •ì œ(clean_name)í•œ ë’¤ count ë¥¼ í•©ì‚°í•´
        output_csv ë¡œ ë‹¤ì‹œ ì €ì¥í•©ë‹ˆë‹¤.
        """
        ctr = Counter()
        with open(input_csv, encoding='utf-8-sig') as f:
            next(f)  # í—¤ë” ê±´ë„ˆë›°ê¸°
            for line in f:
                line = line.strip()
                if not line:
                    continue
                # ë’¤ì—ì„œ í•œ ë²ˆë§Œ ë‚˜ëˆ ì„œ, ì™¼ìª½ì€ place, ì˜¤ë¥¸ìª½ì€ count
                place, cnt = line.rsplit(',', 1)
                place = self.clean_name(place)
                try:
                    num = int(cnt)
                except ValueError:
                    continue
                ctr[place] += num

        # ê²°ê³¼ë¥¼ output_csv ë¡œ ì €ì¥
        with open(output_csv, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['place', 'count'])  # í—¤ë”
            for place, count in ctr.most_common():
                writer.writerow([place, count])

        print(f"âœ… ì¬ì§‘ê³„ ì™„ë£Œ: {output_csv}")

    def extract_from_file(self, path: Path):
        def safe_text(x):
            if isinstance(x, str):
                return x.replace('\x00', '').replace('\ufffd', '').strip()
            return str(x).strip()

        ext = path.suffix.lower()
        try:
            if ext in ('.xls', '.xlsx'):
                wb = openpyxl.load_workbook(path, data_only=True)
                visible_sheets = [ws for ws in wb.worksheets if ws.sheet_state == 'visible']

                if not visible_sheets:
                    print(f"âš ï¸ ë³´ì´ëŠ” ì‹œíŠ¸ ì—†ìŒ: {path.name}")
                    return

                ws = visible_sheets[0]  # ì²« ë²ˆì§¸ ë³´ì´ëŠ” ì‹œíŠ¸ë§Œ ì½ê¸°

                data = []
                for row in ws.iter_rows(values_only=True):
                    if all(cell is None for cell in row):
                        continue
                    data.append([safe_text(cell) for cell in row])

                df = pd.DataFrame(data)
                print(f"âœ… ì—´ëŒ ì„±ê³µ: {path.name}")

                # ì»¬ëŸ¼ ì§ì ‘ íƒìƒ‰
                found = False
                for idx, row in df.iterrows():
                    if any(any(k in str(cell) for k in ['ì‚¬ìš©ì²˜', 'ì‚¬ìš©ì¥ì†Œ', 'ì¥ì†Œ']) for cell in row):
                        df.columns = row
                        df = df.drop(index=range(idx + 1))
                        print(f"ğŸ“Œ ì»¬ëŸ¼ (row {idx}): {df.columns.tolist()}")
                        found = True
                        break

                if not found:
                    print(f"âš ï¸ ì‚¬ìš©ì²˜/ì‚¬ìš©ì¥ì†Œ ì»¬ëŸ¼ ì—†ìŒ: {path.name}")
                    return

                cols = [c for c in df.columns if any(k in safe_text(c) for k in ['ì‚¬ìš©ì²˜', 'ì‚¬ìš©ì¥ì†Œ', 'ì¥ì†Œ'])]

                for col in cols:
                    for v in df[col].dropna().astype(str):
                        name = self.clean_name(v)
                        print(f"ğŸ“ ì¶”ì¶œëœ ì´ë¦„: {name}")
                        if 1 < len(name) < 50:
                            self.counter[name] += 1

            elif ext == '.pdf':
                if path.stat().st_size < 1024:
                    print(f"âš ï¸ {path} íŒŒì¼ ì‚¬ì´ì¦ˆ ë„ˆë¬´ ì‘ìŒ (1KB ì´í•˜)")
                    return
                with pdfplumber.open(path) as pdf:
                    for pg in pdf.pages:
                        for tbl in pg.extract_tables() or []:
                            df = pd.DataFrame(tbl[1:], columns=[safe_text(c) for c in tbl[0]])
                            for col in df.columns:
                                if any(k in safe_text(col) for k in ['ì‚¬ìš©ì²˜', 'ì‚¬ìš©ì¥ì†Œ', 'ì¥ì†Œ']):
                                    for v in df[col].dropna().astype(str):
                                        name = self.clean_name(v)
                                        if 1 < len(name) < 50:
                                            self.counter[name] += 1

            elif ext == '.hwpx':
                tmp = self.download_dir / '_hwpx'
                with zipfile.ZipFile(path) as zp:
                    zp.extractall(tmp)
                xmlf = tmp / 'Contents' / 'section0.xml'
                tree = ET.parse(xmlf)
                for t in tree.iter():
                    if t.tag.endswith('}t') and t.text:
                        text = safe_text(t.text)
                        if any(k in text for k in ['ì‚¬ìš©ì²˜', 'ì‚¬ìš©ì¥ì†Œ', 'ì¥ì†Œ']):
                            name = self.clean_name(text)
                            if 1 < len(name) < 50:
                                self.counter[name] += 1
                shutil.rmtree(tmp)

            else:
                print(f"Unsupported format: {path}")

        except Exception as e:
            print(f"Error parsing {path.name} ({ext}): {e}")

    def save_to_csv(self, output_path: str):
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['place', 'count'])
            for place, count in sorted(self.counter.items(), key=lambda x: -x[1]):
                writer.writerow([self.safe_text(place), count])
        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {output_path}")
    def run(self, max_pages: int = None):
        pages = self.config.pages if max_pages is None else range(1, max_pages+1)
        for p in pages:
            try:
                list_url = self.config.list_url_template.format(page=p)
                print(f"Crawling list page: {list_url}")
                html = self.fetch_html(list_url)
                if not html: continue

                links = self.parser.extract_post_links(html)
                for post in links:
                    try:
                        print(f"Visiting post: {post}")
                        self.driver.get(post)
                        time.sleep(1)
                        page_html = self.driver.page_source
                        attaches = self.parser.extract_attachment_links(page_html)
                        for url, hint in attaches:
                            try:
                                fpath = self.download_file(url, hint)
                                if fpath:
                                    try:
                                        self.extract_from_file(fpath)
                                    finally:
                                        # íŒŒì¼ ì½ë“  ì‹¤íŒ¨í•˜ë“  ë¬´ì¡°ê±´ ì‚­ì œ
                                        fpath.unlink(missing_ok=True)  # Path ê°ì²´ë¼ unlink()
                            except Exception as e:
                                print(f"Error file Pathe {fpath} : {e}")
                    except Exception as e:
                        print(f"Error Post : {e}")
            except Exception as e:
                print(f"Error page : {e}")
        # save report
        df = pd.DataFrame(self.counter.most_common(), columns=['place','count'])
        df.to_csv(
            self.config.output_csv,
            index=False,
            encoding='utf-8-sig',
            quoting=csv.QUOTE_NONE,  # ë”°ì˜´í‘œ ê°ì‹¸ê¸° ì—†ì´
            escapechar='\\'  # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì§€ì •
        )
        print(f"Report saved to {self.config.output_csv}")

    def cleanup(self):
        if self.download_dir.exists():
            shutil.rmtree(self.download_dir)
            print(f"Cleaned up download dir: {self.download_dir}")

class GwangjuCrawler:
    def __init__(self, download_dir: str = "temp/ê´‘ì£¼ì‹œì²­"):
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(parents=True, exist_ok=True)

        chrome_options = Options()
        prefs = {
            "download.default_directory": str(self.download_dir.resolve()),
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "safebrowsing.enabled": True
        }
        chrome_options.add_experimental_option("prefs", prefs)
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--disable-gpu")
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        self.download_dir = Path("temp") / "ê´‘ì£¼ì‹œì²­"
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.counter = Counter()

    def run(self, max_page=1):
        for page in range(1, max_page + 1):
            list_url = f"https://www.gjcity.go.kr/portal/bbs/list.do?ptIdx=53&mId=0311000000&page={page}"
            print(f"Crawling list page: {list_url}")
            self.driver.get(list_url)
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            rows = soup.select("table.bod_list tbody tr")

            for row in rows:
                a_tag = row.select_one("td.list_tit a")
                onclick = a_tag.get("onclick", "")
                bidx = self.extract_bidx_from_onclick(onclick)
                if not bidx:
                    continue
                post_url = f"https://www.gjcity.go.kr/portal/bbs/view.do?bIdx={bidx}&ptIdx=53&mId=0311000000"
                print(f"Visiting post: {post_url}")
                self.driver.get(post_url)
                time.sleep(1.5)

                self.download_attachments_from_page()


    def extract_bidx_from_onclick(self, onclick: str) -> str:
        try:
            return onclick.split("'")[3]
        except IndexError:
            return None

    def download_attachments_from_page(self):
        file_links = self.driver.find_elements(By.CSS_SELECTOR, "ul.file_list li a.download")
        for a in file_links:
            try:
                filename = a.text.strip()
                print(f"ğŸ“ í´ë¦­ ì¤‘: {filename}")
                before_files = set(self.download_dir.glob("*"))
                a.click()
                self.wait_for_download(before_files)
            except Exception as e:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

    def wait_for_download(self, before_files: set, timeout=15):
        for _ in range(timeout):
            after_files = set(self.download_dir.glob("*"))
            new_files = after_files - before_files
            if new_files:
                for file in new_files:
                    print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file.name}")
                return
            time.sleep(1)
        print("âŒ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")

    def close(self):
        self.driver.quit()

def run_geumcheon():
    config = [SiteConfig(
        name="ê¸ˆì²œêµ¬ì²­",
        list_url_template=(
            "https://www.geumcheon.go.kr/portal/selectBbsNttList.do"
            "?bbsNo=86&key=269&id=&searchCtgry=&pageUnit=10"
            "&searchCnd=all&searchKrwd=&integrDeptCode="
            "&searchDeptCode=&pageIndex={page}"
        ),
        # '/portal'ê¹Œì§€ ê¼­ í¬í•¨
        detail_base="https://www.geumcheon.go.kr/portal/",
        row_selector="table.p-table.simple tbody tr",
        title_selector="td.p-subject a",
        attach_selector="a.p-attach__link",
        pages=range(18, 83),
        output_csv="ê¸ˆì²œêµ¬ì²­_ì—…ë¬´ì¶”ì§„ë¹„.csv"
    ),
        SiteConfig(
            name="êµ¬ë¡œêµ¬ì²­",
            list_url_template=(
                "https://www.guro.go.kr/www/selectBbsNttList.do?"
                "bbsNo=655&pageUnit=10&key=1732&pageIndex={page}"
            ),
            # '/portal'ê¹Œì§€ ê¼­ í¬í•¨
            detail_base="https://www.guro.go.kr/www/",
            row_selector="table.p-table.simple tbody tr",
            title_selector="td.p-subject a",
            attach_selector="a.p-attach__link",
            pages=range(20, 100),
            output_csv="êµ¬ë¡œêµ¬ì²­_ì—…ë¬´ì¶”ì§„ë¹„.csv"
        ),
    ]
    guro_config = config[1]
    # 18, 83 -> 2024.01~12 ê¹Œì§€ì˜ page ë²”ìœ„
    parser = DefaultParser(guro_config)
    crawler = Crawler(guro_config, parser)
    crawler.run(max_pages=None)
    crawler.cleanup()
    # crawler.reaggregate_csv('êµ¬ë¡œì²­_ì‹ë‹¹_ì‚¬ìš©ë¹ˆë„.csv', 'êµ¬ë¡œì²­_ì‹ë‹¹_ì‚¬ìš©ë¹ˆë„_ì •ì œ.csv')
    # crawler.reaggregate_csv('ê¸ˆì²œêµ¬ì²­_ì—…ë¬´ì¶”ì§„ë¹„.csv', 'ê¸ˆì²œêµ¬ì²­_ì—…ë¬´ì¶”ì§„ë¹„_ì •ì œ2.csv')

def run_mapo():
    mapo_config = SiteConfig(
        name='ë§ˆí¬êµ¬ì²­',
        list_url_template=(
            'https://www.mapo.go.kr/site/main/board/expense/list'
            '?cp={page}&sortOrder=BA_REGDATE'
            '&sortDirection=DESC'
            '&listType=list'
            '&bcId=expense'
            '&baUse=true'
        ),
        detail_base='https://www.mapo.go.kr/site/main/',
        row_selector='',  # DefaultParserëŠ” ì‚¬ìš© ì•ˆ í•¨
        title_selector='',
        attach_selector='',
        pages=range(20, 91),
        output_csv='ë§ˆí¬êµ¬ì²­_ì—…ë¬´ì¶”ì§„ë¹„.csv'
    )
    # 20, 91
    parser = MapoParser(mapo_config)
    crawler = Crawler(mapo_config, parser)
    crawler.run(max_pages=100)
    crawler.download_dir


def run_gwangju():
    config = SiteConfig(
        name="ê´‘ì£¼ì‹œì²­",
        list_url_template="https://www.gjcity.go.kr/portal/bbs/list.do?ptIdx=53&mId=0311000000&page={page}",
        detail_base="https://www.gjcity.go.kr",
        row_selector="table.bod_list tbody tr",
        title_selector="td.list_tit a",
        attach_selector="ul.file_list li a.download",
        pages=range(15, 73),
        output_csv="ê´‘ì£¼ì‹œì²­_ì—…ë¬´ì¶”ì§„ë¹„.csv"
    )
    parser = GwangjuParser(config)
    crawler = Crawler(config, parser)

    for page in config.pages:
        list_url = config.list_url_template.format(page=page)
        html = crawler.fetch_html(list_url)
        if not html:
            continue
        posts = parser.extract_post_links(html)
        for url in posts:
            print(f"Visiting post: {url}")
            print(f"Visiting post: {url}")
            crawler.driver.get(url)
            files = parser.selenium_download_attachments(crawler.driver, crawler.download_dir)
            for f in files:
                crawler.extract_from_file(f)
                f.unlink(missing_ok=True)  # Path ê°ì²´ë¼ unlink()


    # save
    df = pd.DataFrame(crawler.counter.most_common(), columns=['place', 'count'])
    df.to_csv(config.output_csv, index=False, encoding='utf-8-sig')
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {config.output_csv}")


if __name__ == '__main__':
    run_gwangju()
