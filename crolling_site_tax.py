# êµ¬ë¡œêµ¬ì²­ ì—…ë¬´ì¶”ì§„ë¹„ ì²¨ë¶€íŒŒì¼ í¬ë¡¤ë§ ë° íŒŒì‹± ë¦¬íŒ©í„°ë§ ì½”ë“œ
from urllib.parse import unquote
from pathlib import Path

import os
import time
import requests
import pandas as pd
import re
import shutil
from pathlib import Path
import zipfile
import pdfplumber
from io import BytesIO
from bs4 import BeautifulSoup
from collections import Counter
from urllib.parse import urljoin
import xml.etree.ElementTree as ET
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import urllib.parse
from urllib.parse import unquote

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# ì„¤ì •
save_dir  = "temp"
os.makedirs(save_dir, exist_ok=True)
headers = {"User-Agent": "Mozilla/5.0"}
restaurant_list = []
restaurant_counter = Counter()

# Selenium ë“œë¼ì´ë²„ ì„¤ì •
options = Options()
options.add_argument("--headless")
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


def clean_restaurant_name(name):
    # (ì£¼) ë˜ëŠ” ãˆœ ì œê±°
    name = re.sub(r"\(?ãˆœ?\s?ì£¼ì‹íšŒì‚¬\)?", "", name)
    name = re.sub(r"\(?ãˆœ?\)?", "", name)
    name = re.sub(r"\(?ì£¼\)?", "", name)

    # ê´„í˜¸ ì•ˆ í…ìŠ¤íŠ¸ ì œê±°
    name = re.sub(r"\(.*?\)", "", name)

    # ì•ë’¤ ê³µë°± ì •ë¦¬ ë° ì¤‘ë³µ ê³µë°± ì œê±°
    name = re.sub(r"\s+", " ", name).strip()

    return name
# ------------------------ ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ ------------------------ #
def download_file(file_url, title):
    try:
        response = requests.get(file_url, stream=True)
        response.encoding = 'utf-8'  # í•„ìš”ì‹œ 'euc-kr' ë˜ëŠ” 'cp949'

        # Content-Disposition í—¤ë”ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
        if 'Content-Disposition' in response.headers:
            content_disposition = response.headers['Content-Disposition']
            fname_part = content_disposition.split("filename=")[-1].strip('"')
            try:
                # URL ì¸ì½”ë”©ëœ íŒŒì¼ëª… ë””ì½”ë”© ì‹œë„
                filename = unquote(fname_part.encode('latin1').decode('utf-8'))
            except:
                filename = unquote(fname_part)  # fallback

        else:
            filename = file_url.split("/")[-1]  # fallback

        # íŒŒì¼ ì €ì¥
        save_path = save_dir+"/"+ filename

        with open(save_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {save_path}")
        return save_path

    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_url}, ì—ëŸ¬: {e}")
        return None
def process_file(file_path):
    # ì—¬ê¸°ì— ì‹¤ì œ ì²˜ë¦¬ ì‘ì—… (ì˜ˆ: í…ìŠ¤íŠ¸ ì¶”ì¶œ, ë°ì´í„° ë¶„ì„ ë“±)
    print(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì¤‘... {file_path.name}")
    # ì˜ˆì‹œ ì‘ì—…: ì´ë¦„ ì¶œë ¥
    print(f"ğŸ“‚ íŒŒì¼ ì´ë¦„: {file_path.name}")


def clean_file(file_path):
    try:
        file_path.unlink()
        print(f"ğŸ§¹ ì‚­ì œ ì™„ë£Œ: {file_path}")
    except Exception as e:
        print(f"âš ï¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
def extract_excel(file_path):
    try:
        df = pd.read_excel(file_path, engine='openpyxl' if file_path.endswith('xlsx') else None)
        for _, row in df.iterrows():
            values = row.dropna().astype(str).str.strip()
            for val in values:
                if 1 < len(val) < 50 and not any(k in val for k in ["ì‚¬ìš©ì²˜", "ì‚¬ìš©ì¥ì†Œ","ì¥ì†Œ"]):
                    val = val.replace('"', "")
                    val = val.replace("'", "")
                    restaurant_list.append(val)
        return df
    except Exception as e:
        print(f"âŒ ì—‘ì…€ íŒŒì‹± ì‹¤íŒ¨: {file_path}, ì—ëŸ¬: {e}")
        return pd.DataFrame()

def extract_pdf(file_path):
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                tables = page.extract_tables()
                for table in tables:
                    df = pd.DataFrame(table)
                    df.columns = df.iloc[0]
                    df = df[1:]
                    for col in df.columns:
                        if any(keyword in col for keyword in ["ì‚¬ìš©ì²˜", "ì‚¬ìš©ì¥ì†Œ","ì¥ì†Œ"]):
                            for val in df[col].dropna():
                                val_clean = str(val).strip()
                                val_clean = val_clean.replace('"',"")
                                val_clean = val_clean.replace("'","")
                                if 1 < len(val_clean) < 50:
                                    restaurant_list.append(val_clean)
    except Exception as e:
        print(f"âŒ PDF íŒŒì‹± ì‹¤íŒ¨: {file_path}, ì—ëŸ¬: {e}")

def extract_hwpx(file_path):
    try:
        with zipfile.ZipFile(file_path) as zip_ref:
            zip_ref.extractall("temp_hwpx")
        tree = ET.parse("temp_hwpx/Contents/section0.xml")
        root = tree.getroot()
        for elem in root.iter():
            if elem.tag.endswith("t") and elem.text:
                text = elem.text.strip()
                if any(kw in text for kw in ["ì‚¬ìš©ì²˜", "ì‚¬ìš©ì¥ì†Œ","ì¥ì†Œ"]):
                    if 1 < len(text) < 50:
                        text = text.replace('"', "")
                        text = text.replace("'", "")
                        restaurant_list.append(text)
    except Exception as e:
        print(f"âŒ HWPX íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

def extract_hwp(file_path):
    print(f"ğŸ“„ HWP íŒŒì¼ ì €ì¥ë¨ (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—†ìŒ): {os.path.basename(file_path)}")
    restaurant_list.append({
        "íŒŒì¼": os.path.basename(file_path),
        "ì„¤ëª…": "HWP íŒŒì¼ (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì•ˆë¨)"
    })
# ------------------------ ê²Œì‹œê¸€ ìˆ˜ì§‘ ë° ì²˜ë¦¬ ------------------------ #
def site1_pattern(base_list_url):
    for page in range(20, 100):  # í˜ì´ì§€ ë²”ìœ„ ì¡°ì •
        driver.get(f"{base_list_url}&pageIndex={page}")
        time.sleep(1)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        rows = soup.select("table.p-table.simple tbody tr")

        for row in rows:
            a_tag = row.select_one("td.p-subject a")
            if not a_tag:
                continue

            title = a_tag.get_text(strip=True)
            href = a_tag.get("href")
            detail_url = urljoin("https://www.guro.go.kr/www/", href)

            print(f"ğŸ” ê²Œì‹œê¸€: {title} â†’ {detail_url}")
            res = requests.get(detail_url, headers=headers)
            soup = BeautifulSoup(res.text, "html.parser")
            attach_links = soup.select("a.p-attach__link")
            file_urls = [urljoin("https://www.guro.go.kr/www/", tag["href"]) for tag in attach_links if tag.get("href")]

            for file_url in file_urls:
                file_path = download_file(file_url, title)
                if not file_path:
                    continue
                known_exts = [".xlsx", ".xls", ".pdf", ".hwpx", ".hwp"]
                ext = os.path.splitext(file_path)[1].lower()

                if ext == ".xlsx" or ext == ".xls":
                    extract_excel(file_path)
                elif ext == ".pdf":
                    extract_pdf(file_path)
                elif ext == ".hwpx":
                    extract_hwpx(file_path)
                elif ext == ".hwp":
                    extract_hwp(file_path)
                elif ext not in known_exts:
                    print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹: {file_path}")


            else:
                    print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹: {file_path}")
def delete_folder(folder_path):
    folder = Path(folder_path)
    if folder.exists() and folder.is_dir():
        shutil.rmtree(folder)
        print(f"ğŸ§¹ í´ë” ì „ì²´ ì‚­ì œ ì™„ë£Œ: {folder}")
    else:
        print(f"âš ï¸ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ í´ë”ê°€ ì•„ë‹™ë‹ˆë‹¤: {folder}")
# ------------------------ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥ ------------------------ #
if __name__ == "__main__":
    base_list_url = "https://www.guro.go.kr/www/selectBbsNttList.do?bbsNo=655&&pageUnit=10&key=1732"
    site1_pattern(base_list_url)

    cleaned_restaurants = [r for r in restaurant_list if isinstance(r, str) and len(r.strip()) >= 2]
    cleaned_restaurants = [clean_restaurant_name(name) for name in cleaned_restaurants]
    restaurant_counter = Counter(cleaned_restaurants)
    # ê²°ê³¼ ì €ì¥
    final_df = pd.DataFrame(restaurant_counter.items(), columns=["ì‚¬ìš©ì¥ì†Œ", "íšŸìˆ˜"])
    final_df = final_df.sort_values(by="íšŸìˆ˜", ascending=False)
    final_df.to_csv("êµ¬ë¡œì²­_ì‹ë‹¹_ì‚¬ìš©ë¹ˆë„.csv", index=False, encoding="utf-8-sig")
    print("âœ… ì‹ë‹¹ ì‚¬ìš© ë¹ˆë„ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")

    delete_folder(save_dir)