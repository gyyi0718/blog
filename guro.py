import os
import requests
import pandas as pd
from urllib.parse import urljoin
import pdfplumber
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
SAVE_DIR = "guro_downloads"
os.makedirs(SAVE_DIR, exist_ok=True)
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time

# âœ… Seleniumìœ¼ë¡œ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
def get_post_list_playwright(pages=1):
    post_list = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page(viewport={"width": 1280, "height": 1000})

        for i in range(1, pages + 1):
            url = f"https://www.guro.go.kr/www/selectBbsNttList.do?bbsNo=655&&pageUnit=10&key=1732&pageIndex={i}"
            print(f"â–¶ ì ‘ì† ì¤‘: {url}")
            page.goto(url, timeout=30000)

            # ê°•ì œë¡œ ìŠ¤í¬ë¡¤í•´ì„œ lazy load íŠ¸ë¦¬ê±°
            page.mouse.wheel(0, 3000)
            page.wait_for_timeout(3000)  # ê¸°ë‹¤ë¦¬ê¸°

            try:
                page.wait_for_selector("table.tblList tbody tr", timeout=5000)
            except:
                print(f"â— ê²Œì‹œíŒ í‘œ ì•ˆëœ¸, HTML ì €ì¥")
                with open(f"fail_page_{i}.html", "w", encoding="utf-8") as f:
                    f.write(page.content())
                continue

            html = page.content()
            soup = BeautifulSoup(html, "html.parser")
            rows = soup.select("table.tblList tbody tr")
            print(f"âœ… {i}í˜ì´ì§€ ê²Œì‹œê¸€ {len(rows)}ê°œ íƒìƒ‰")

            for row in rows:
                cols = row.find_all("td")
                if len(cols) < 4:
                    continue
                title = cols[1].get_text(strip=True)
                date = cols[3].get_text(strip=True)
                a_tag = cols[1].find("a")
                if not a_tag or "onclick" not in a_tag.attrs:
                    continue
                try:
                    onclick = a_tag["onclick"]
                    ntt_no = onclick.split("'")[1]
                except:
                    continue
                post_list.append({
                    "ì œëª©": title,
                    "ë“±ë¡ì¼": date,
                    "ê²Œì‹œê¸€ë²ˆí˜¸": ntt_no
                })
            time.sleep(0.5)

        browser.close()

    return post_list
def get_file_links(ntt_no):
    detail_url = "https://www.guro.go.kr/www/selectBbsNttInfo.do"
    params = {
        "bbsNo": 655,
        "key": 1732,
        "nttNo": ntt_no
    }
    res = requests.get(detail_url, params=params)
    soup = BeautifulSoup(res.text, "html.parser")
    links = soup.select("div.bbs_file a")
    return [urljoin("https://www.guro.go.kr", a["href"]) for a in links if a.get("href")]

# âœ… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
def download_file(file_url, title):
    try:
        ext = file_url.split('.')[-1].split('?')[0]
        filename = f"{title.replace(' ', '_')}.{ext}"
        path = os.path.join(SAVE_DIR, filename)
        r = requests.get(file_url)
        with open(path, "wb") as f:
            f.write(r.content)
        return path
    except Exception as e:
        print("âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨:", file_url, e)
        return None

# âœ… ì—‘ì…€ íŒŒì¼ íŒŒì‹±
def parse_excel(file_path):
    try:
        df = pd.read_excel(file_path, engine='openpyxl' if file_path.endswith('xlsx') else None)
        df["ì¶œì²˜íŒŒì¼"] = os.path.basename(file_path)
        return df
    except Exception as e:
        print(f"âŒ ì—‘ì…€ íŒŒì‹± ì‹¤íŒ¨: {file_path}, {e}")
        return pd.DataFrame()

# âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def parse_pdf(file_path):
    try:
        with pdfplumber.open(file_path) as pdf:
            text = "\n".join([page.extract_text() or '' for page in pdf.pages])
        return {"íŒŒì¼": os.path.basename(file_path), "ë¯¸ë¦¬ë³´ê¸°": text[:500]}
    except Exception as e:
        print(f"âŒ PDF íŒŒì‹± ì‹¤íŒ¨: {file_path}, {e}")
        return None

# âœ… HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ë³¸ì€ ì´ë¦„ë§Œ ê¸°ë¡)
def parse_hwp(file_path):
    # hwp íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œì€ ë³µì¡í•´ì„œ ì™¸ë¶€ ë„êµ¬ í•„ìš” (ì—¬ê¸°ì„  ì´ë¦„ë§Œ ìˆ˜ì§‘)
    return {"íŒŒì¼": os.path.basename(file_path), "ë¯¸ë¦¬ë³´ê¸°": "â€» HWP íŒŒì¼ â€“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ìƒëµë¨"}

# âœ… ì „ì²´ ì²˜ë¦¬
def run_all(pages=3):
    posts = get_post_list_playwright(pages)
    excel_list = []
    pdf_previews = []
    hwp_infos = []

    for post in posts:
        print(f"ğŸ“„ {post['ì œëª©']} ì²˜ë¦¬ ì¤‘...")
        links = get_file_links(post['ê²Œì‹œê¸€ë²ˆí˜¸'])
        for link in links:
            path = download_file(link, post["ì œëª©"])
            if not path:
                continue
            if path.endswith((".xls", ".xlsx")):
                df = parse_excel(path)
                if not df.empty:
                    excel_list.append(df)
            elif path.endswith(".pdf"):
                preview = parse_pdf(path)
                if preview:
                    pdf_previews.append(preview)
            elif path.endswith(".hwp"):
                hwp_infos.append(parse_hwp(path))
            time.sleep(0.3)

    # ê²°ê³¼ ì €ì¥
    if excel_list:
        merged_df = pd.concat(excel_list, ignore_index=True)
        merged_df.to_excel("guro_ì—…ë¬´ì¶”ì§„ë¹„_í†µí•©.xlsx", index=False)
        print("âœ… ì—‘ì…€ í†µí•© ì™„ë£Œ: guro_ì—…ë¬´ì¶”ì§„ë¹„_í†µí•©.xlsx")

    if pdf_previews:
        pd.DataFrame(pdf_previews).to_excel("guro_pdf_ë¯¸ë¦¬ë³´ê¸°.xlsx", index=False)
        print("âœ… PDF ìš”ì•½ ì €ì¥ ì™„ë£Œ")

    if hwp_infos:
        pd.DataFrame(hwp_infos).to_excel("guro_hwp_íŒŒì¼ëª©ë¡.xlsx", index=False)
        print("âœ… HWP ëª©ë¡ ì €ì¥ ì™„ë£Œ")

get_post_list_playwright(3)