import requests
from bs4 import BeautifulSoup
from openai import OpenAI
from datetime import datetime
import time
import hot_topic as ht



# âœ… OpenAI API í‚¤ ì…ë ¥
client = OpenAI(api_key="my_key")



# âœ… ì‹¤ì œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
def get_articles(rss_url, max_count=5):
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(rss_url, headers=headers)
    soup = BeautifulSoup(res.content, "xml")

    items = soup.find_all("item")[:max_count]
    articles = []

    for item in items:
        title = item.title.text
        link_tag = item.find("link")
        source_tag = item.find("source")  # âœ… ì›ë¬¸ URLì¼ ìˆ˜ ìˆìŒ
        guid_tag = item.find("guid")

        # âœ… 1ìˆœìœ„: <source url=""> ì†ì„±
        if source_tag and source_tag.has_attr("url"):
            real_url = source_tag["url"]
        # âœ… 2ìˆœìœ„: <guid> ê°’ì´ ì‹¤ì œ URLì¸ ê²½ìš°
        elif guid_tag and guid_tag.text.startswith("http"):
            real_url = guid_tag.text
        # âœ… 3ìˆœìœ„: fallback - link ê°’
        else:
            real_url = link_tag.text

        articles.append({"title": title, "link": real_url})

    return articles

# âœ… ë³¸ë¬¸ ì¶”ì¶œ
def get_article_text_bs4(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=10)
        res.encoding = res.apparent_encoding
        #res.encoding = "utf-8"
        soup = BeautifulSoup(res.text, 'html.parser')

        if "donga.com" in url:
            body = soup.select_one('.article_txt')
        elif "ohmynews.com" in url:
            wrapper = soup.select_one('div#article_view_content') or soup.select_one('div#oArticle')
            if wrapper:
                paragraphs = wrapper.find_all("p")
                text = " ".join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30)
                return text if text else None
        elif "nate.com" in url:
            body = soup.select_one('#realArtcContents') or soup.select_one('.articleCont')
        elif "mk.co.kr" in url:
            body = soup.select_one('#article_body')
        elif "xportsnews.com" in url:
            body = soup.select_one('.articalContent') or soup.select_one('#articleBody')
        else:
            paragraphs = soup.find_all("p")
            text = " ".join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30)
            return text if text else None

        return body.get_text(strip=True) if body else None
    except Exception as e:
        return None

# âœ… GPT ìš”ì•½
def summarize_article(title, text):
    if not text or len(text) < 200:
        return "[âŒ ë³¸ë¬¸ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ì¶”ì¶œ ì‹¤íŒ¨]"

    prompt = f"""ì œëª©: {title}
ë‹¤ìŒ ê¸°ì‚¬ë¥¼ ë¸”ë¡œê·¸ í˜•ì‹ìœ¼ë¡œ 3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜:\n{text[:3000]}"""

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"[ìš”ì•½ ì‹¤íŒ¨] {str(e)}"

# âœ… ë¸”ë¡œê·¸ ê¸€ ìƒì„±
def create_blog_post(topic_summaries):
    date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    content = f"# ì˜¤ëŠ˜ì˜ ìœ¡ì•„Â·ì¬í…Œí¬ íŠ¸ë Œë“œ ìš”ì•½ - {date}\n\n"
    content += "ìœ¡ì•„ ì¤‘ì—ë„ ê²½ì œ ê°ê°ê³¼ ìë…€ êµìœ¡ ê°ê°, ë‘˜ ë‹¤ ì±™ê²¨ë³´ì•„ìš” ğŸ˜Š\n\n"

    for category, title, summary in topic_summaries:
        content += f"## ğŸ“Œ [{category}] {title}\n{summary}\n\n"

    content += "---\nì˜¤ëŠ˜ë„ ìŒë‘¥ì´ ë‚®ì  ì‹œê°„ì— ì •ë¦¬í•´ë´¤ì–´ìš”. ë‚´ì¼ë„ í•¨ê»˜ ê³µë¶€í•´ìš”!"
    return content

# âœ… ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜
def generate_daily_post(rss_urls):
    topic_summaries = []
    for category, rss_url in rss_urls.items():
        articles = get_articles(rss_url, max_count=3)
        for article in articles:
            print(f"{category} - {article['title']}")
            text = get_article_text_bs4(article['link'])
            summary = summarize_article(article['title'], text)
            topic_summaries.append((category, article['title'], summary))
            time.sleep(2)

    post = create_blog_post(topic_summaries)
    file_name = f"blog_post_{datetime.now().strftime('%Y%m%d')}.txt"

    with open(file_name, "a", encoding="utf-8") as f:
        f.write(post)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {file_name}")

# â–¶ ì‹¤í–‰
if __name__ == "__main__":
    # âœ… RSS í”¼ë“œ (ìœ¡ì•„ / ì¬í…Œí¬)]
    topic_arr = ["ë‰´ìŠ¤","ìœ¡ì•„","ì¬í…Œí¬"]

    for t in topic_arr:
        top_keyword = ht.search_keyword_trends(t)
        for top_key in top_keyword:
            rss_urls = {"{}": "https://news.google.com/rss/search?q={}&hl=ko&gl=KR&ceid=KR:ko".format(t,top_key)}

        generate_daily_post(rss_urls)
